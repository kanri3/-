# 応用数学レポート

☆第1章：線形代数
1) 固有値・固有ベクトルの求め方を確認する。
2) 固有値分解について理解を深める。
3) 特異値・特異ベクトルの概要を知る
4) 特異値分解の概要を知る。

☆第2章：確率・統計
1) 条件付き確率とは？  
事象X=xを前提に事象Y=yが生じる確率。  
ここでX,Yはある「系」（サイコロ，天気etc）を表し、またx,yはその系の下で起きる事象を表す。
2) ベイズの定理とは？  
たとえば「ワクチン接種をすると安心顔になる確率」が既知であるなら、ベイズの定理により「安心顔の人がワクチン接種済みである確率」を求められる。  
（但し、「ワクチン接種を受けられる確率」および「安心顔である確率」が既知であること）
3) 期待値とは？  
いわゆる平均値。  
確率変数と確率の積を全ての事象について合計したもの。
☆第2章:確率・統計
1) 条件付き確率とは？
事象X=xを前提に事象Y=yが生じる確率。
ここでX,Yは「ある系」（サイコロ，天気etc）を表し、またx,yはその系の下で起きる事象を表す。
2) ベイズの定理とは？
たとえばワクチン接種をすると安心顔になる確率が既知であるなら、ベイズ則により安心顔の人がワクチン接種を済ましている確率を求められる。
（但し、ワクチン接種を受けられる確率および安心顔である確率が既知であること）
3) 期待値とは？
いわゆる平均値。
確率変数と確率の積を全ての事象について合計したもの。
4) 分散とは？  
分散とは、データの散らばり具合を表す値である。  
データの正負が原因で生じる相殺を回避する処理（二乗）の後、平均をとったもの。  
5) 様々な確率分布の概要を知る。

☆第3章：情報理論
1) 自己情報量とは？  
事象が発生する前後で不確かさ（エントロピー）が減少する場合、その差を自己情報量という。
2) 平均情報量（シャノンエントロピー）とは？  
自己情報量の期待値である。
3) KLダイバージェンスとは？  
２つの確率分布がどの程度似ているかを表す尺度。
4) 交差エントロピー誤差とは？  
機械学習で推定した確率分布が、真の確率分布に近い程小さくなる値。
